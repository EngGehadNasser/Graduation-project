{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11050092,"sourceType":"datasetVersion","datasetId":6883985},{"sourceId":11095295,"sourceType":"datasetVersion","datasetId":6916491}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport os\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom flask import Flask, request, jsonify\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T01:11:34.399874Z","iopub.execute_input":"2025-03-20T01:11:34.400085Z","iopub.status.idle":"2025-03-20T01:11:57.333303Z","shell.execute_reply.started":"2025-03-20T01:11:34.400065Z","shell.execute_reply":"2025-03-20T01:11:57.332438Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def clean_text(text):\n    text = str(text)  \n    text = text.lower()  \n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  \n    text = re.sub(r\"@\\w+|#\", '', text)  \n    text = re.sub(r\"[^\\w\\s]\", '', text) \n    text = re.sub(r\"\\d+\", '', text)  \n    text = re.sub(r\"\\s+\", ' ', text).strip()  \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T01:11:57.334588Z","iopub.execute_input":"2025-03-20T01:11:57.335477Z","iopub.status.idle":"2025-03-20T01:11:57.341297Z","shell.execute_reply.started":"2025-03-20T01:11:57.335443Z","shell.execute_reply":"2025-03-20T01:11:57.340314Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ndef load_data(file_path, lang):\n    df = pd.read_csv(file_path)\n\n    if lang == 'ar':\n        text_col, label_col = 'review_description', 'rating'  \n    else:\n        text_col, label_col = 'Summary', 'Sentiment' \n\n    if label_col in df.columns:\n        label_mapping = {'positive': 1, 'negative': 0, 'neutral': 2}  \n        if df[label_col].dtype == object:  \n            df[label_col] = df[label_col].map(label_mapping)\n\n    df = df[[text_col, label_col]].dropna()\n\n    df[text_col] = df[text_col].apply(clean_text)\n\n    return df.rename(columns={text_col: 'text', label_col: 'label'})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T01:11:57.342384Z","iopub.execute_input":"2025-03-20T01:11:57.342594Z","iopub.status.idle":"2025-03-20T01:11:57.367044Z","shell.execute_reply.started":"2025-03-20T01:11:57.342575Z","shell.execute_reply":"2025-03-20T01:11:57.366194Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def tokenize_function(examples, tokenizer):\n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    return {'accuracy': accuracy_score(labels, predictions)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T01:11:57.367866Z","iopub.execute_input":"2025-03-20T01:11:57.368235Z","iopub.status.idle":"2025-03-20T01:11:57.381927Z","shell.execute_reply.started":"2025-03-20T01:11:57.368205Z","shell.execute_reply":"2025-03-20T01:11:57.381157Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def fine_tune_model(data_file, model_name, lang, output_dir):\n    df = load_data(data_file, lang)  \n    # df = df[:1000]  \n    df = load_data(data_file, lang)\n    train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n    \n    train_dataset = Dataset.from_dict({'text': train_texts.tolist(), 'label': train_labels.tolist()})\n    val_dataset = Dataset.from_dict({'text': val_texts.tolist(), 'label': val_labels.tolist()})\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    train_dataset = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n    val_dataset = val_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n    \n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n    \n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        eval_strategy='epoch',\n        save_strategy='epoch',\n        learning_rate=2e-5,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        num_train_epochs=1,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n        save_total_limit=2\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    import traceback\n    try:\n        trainer.train()\n    except Exception as e:\n        print(\"Error:\", e)\n    traceback.print_exc()\n\n    trainer.save_model(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    return trainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T01:11:57.383961Z","iopub.execute_input":"2025-03-20T01:11:57.384212Z","iopub.status.idle":"2025-03-20T01:11:57.396392Z","shell.execute_reply.started":"2025-03-20T01:11:57.384180Z","shell.execute_reply":"2025-03-20T01:11:57.395641Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T01:11:57.397412Z","iopub.execute_input":"2025-03-20T01:11:57.397619Z","iopub.status.idle":"2025-03-20T01:11:57.417900Z","shell.execute_reply.started":"2025-03-20T01:11:57.397601Z","shell.execute_reply":"2025-03-20T01:11:57.417110Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pdatasets1/Final_Data.csv\n/kaggle/input/pdatasets1/Dataset-SA.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-tuning models\nimport wandb\nwandb.login(key=\"0c98741e2c5633554723d3bc3e0b466aa6c08f2f\") \narabic_results = fine_tune_model('/kaggle/input/pdatasets1/Final_Data.csv', 'aubmindlab/bert-base-arabertv02', 'ar', '/kaggle/working/arabic_model')\nenglish_results = fine_tune_model('/kaggle/input/pdatasets1/Dataset-SA.csv', 'bert-base-uncased', 'en', '/kaggle/working/english_model')\n\nprint(\"Arabic Model Accuracy:\", arabic_results['eval_accuracy'])\nprint(\"English Model Accuracy:\", english_results['eval_accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T01:11:57.418660Z","iopub.execute_input":"2025-03-20T01:11:57.418937Z","iopub.status.idle":"2025-03-20T04:43:55.000872Z","shell.execute_reply.started":"2025-03-20T01:11:57.418916Z","shell.execute_reply":"2025-03-20T04:43:54.999807Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhabibaahmad2255\u001b[0m (\u001b[33mhabibaahmad2255-fayoum-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/381 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44b641c037c4b0abd0b02b6b62043fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80e446daf87d422ea8379dc258882b92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/825k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84c0cf0a94e841559b9ea91092351220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.64M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4999d4c874e41ac91013691130caa94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3571280ed04a40cdac6a7b86abd12ca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/32036 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ce442dabdf44f0923238e50c08a107"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"230a0792c29a4d92aec113610f1a1940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d31451e78bec44f78da23925a35b296e"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-5-6bd6ca07bb51>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250320_011231-jwecuky9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/habibaahmad2255-fayoum-university/huggingface/runs/jwecuky9' target=\"_blank\">/kaggle/working/arabic_model</a></strong> to <a href='https://wandb.ai/habibaahmad2255-fayoum-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/habibaahmad2255-fayoum-university/huggingface' target=\"_blank\">https://wandb.ai/habibaahmad2255-fayoum-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/habibaahmad2255-fayoum-university/huggingface/runs/jwecuky9' target=\"_blank\">https://wandb.ai/habibaahmad2255-fayoum-university/huggingface/runs/jwecuky9</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4005' max='4005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4005/4005 32:29, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.442300</td>\n      <td>0.419993</td>\n      <td>0.861656</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"NoneType: None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1002' max='1002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1002/1002 02:04]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"648848875ad94488a21a9facb8e09d03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3c7392a1a3844b689147f7b6d5737b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea67c57f256641e1b5401073ec180ff9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0260e06c890e43fb941b28ad5b0bb917"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/164032 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b851713051c646b2aba40cdd5ae83069"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/41009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d72ba2d7bae34862a89b384d5e8421b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9715bfeba2c146efa610e9282d1e2192"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-5-6bd6ca07bb51>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20504' max='20504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20504/20504 2:44:45, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.112400</td>\n      <td>0.164860</td>\n      <td>0.953205</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"NoneType: None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5127' max='5127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5127/5127 10:43]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Arabic Model Accuracy: 0.8616556374079161\nEnglish Model Accuracy: 0.953205393937916\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!zip -r /kaggle/working/arabic_model.zip /kaggle/working/arabic_model\n!zip -r /kaggle/working/english_model.zip /kaggle/working/english_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T04:43:55.001989Z","iopub.execute_input":"2025-03-20T04:43:55.002228Z","iopub.status.idle":"2025-03-20T04:46:56.020237Z","shell.execute_reply.started":"2025-03-20T04:43:55.002205Z","shell.execute_reply":"2025-03-20T04:46:56.019477Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/arabic_model/ (stored 0%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/ (stored 0%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/tokenizer.json (deflated 74%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/config.json (deflated 51%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/scheduler.pt (deflated 56%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/model.safetensors (deflated 7%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/tokenizer_config.json (deflated 78%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/optimizer.pt (deflated 33%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/special_tokens_map.json (deflated 80%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/rng_state.pth (deflated 25%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/trainer_state.json (deflated 82%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/training_args.bin (deflated 52%)\n  adding: kaggle/working/arabic_model/checkpoint-4005/vocab.txt (deflated 65%)\n  adding: kaggle/working/arabic_model/tokenizer.json (deflated 74%)\n  adding: kaggle/working/arabic_model/config.json (deflated 51%)\n  adding: kaggle/working/arabic_model/model.safetensors (deflated 7%)\n  adding: kaggle/working/arabic_model/tokenizer_config.json (deflated 78%)\n  adding: kaggle/working/arabic_model/special_tokens_map.json (deflated 80%)\n  adding: kaggle/working/arabic_model/training_args.bin (deflated 52%)\n  adding: kaggle/working/arabic_model/vocab.txt (deflated 65%)\n  adding: kaggle/working/english_model/ (stored 0%)\n  adding: kaggle/working/english_model/tokenizer.json (deflated 71%)\n  adding: kaggle/working/english_model/config.json (deflated 51%)\n  adding: kaggle/working/english_model/model.safetensors (deflated 7%)\n  adding: kaggle/working/english_model/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/english_model/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/english_model/checkpoint-20504/ (stored 0%)\n  adding: kaggle/working/english_model/checkpoint-20504/tokenizer.json (deflated 71%)\n  adding: kaggle/working/english_model/checkpoint-20504/config.json (deflated 51%)\n  adding: kaggle/working/english_model/checkpoint-20504/scheduler.pt (deflated 56%)\n  adding: kaggle/working/english_model/checkpoint-20504/model.safetensors (deflated 7%)\n  adding: kaggle/working/english_model/checkpoint-20504/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/english_model/checkpoint-20504/optimizer.pt (deflated 22%)\n  adding: kaggle/working/english_model/checkpoint-20504/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/english_model/checkpoint-20504/rng_state.pth (deflated 25%)\n  adding: kaggle/working/english_model/checkpoint-20504/trainer_state.json (deflated 81%)\n  adding: kaggle/working/english_model/checkpoint-20504/training_args.bin (deflated 51%)\n  adding: kaggle/working/english_model/checkpoint-20504/vocab.txt (deflated 53%)\n  adding: kaggle/working/english_model/training_args.bin (deflated 51%)\n  adding: kaggle/working/english_model/vocab.txt (deflated 53%)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Flask App\napp = Flask(__name__)\n\ndef load_pipeline(model_path):\n    return pipeline(\"text-classification\", model=model_path, tokenizer=model_path)\n\narabic_classifier = load_pipeline(\"/kaggle/working/arabic_model\")\nenglish_classifier = load_pipeline(\"/kaggle/working/english_model\")\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.json\n    text = data.get(\"text\", \"\")\n    lang = data.get(\"lang\", \"en\")\n    \n    if lang == 'ar':\n        result = arabic_classifier(text)\n    else:\n        result = english_classifier(text)\n    \n    return jsonify(result)\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T04:46:56.021288Z","iopub.execute_input":"2025-03-20T04:46:56.021542Z","execution_failed":"2025-03-20T05:58:58.385Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":" * Serving Flask app '__main__'\n * Debug mode: off\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/working\"))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-20T05:58:58.386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-20T05:58:58.386Z"}},"outputs":[],"execution_count":null}]}